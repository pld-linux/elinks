--- elinks/src/protocol/http/http.c	Sun Dec 22 12:52:20 2002
+++ elinks.2/src/protocol/http/http.c	Tue Dec 24 22:15:59 2002
@@ -665,16 +665,14 @@
 	/* Number of uncompressed bytes that could be safely get from
 	 * read_encoded().  We can't want to read too much, because if gzread
 	 * "clears" the buffer next time it will return -1. */
-	int ret = 0;
+	int ret = 2048;
 	int r = 0, *length_of_block;
-	/* If true, all stuff was written to pipe and only uncompression is
-	 * wanted now. */
-	int finishing = 0;
 	unsigned char *output = NULL;
 
 	length_of_block = (info->length == LEN_CHUNKED ? &info->chunk_remaining
 						       : &info->length);
-	if (!*length_of_block) finishing = 1;
+	if (!*length_of_block)
+		ret = 65536;
 
 	if (conn->content_encoding == ENCODING_NONE) {
 		*new_len = len;
@@ -690,9 +688,9 @@
 		|| set_nonblocking_fd(conn->stream_pipes[1]) < 0))
 		return NULL;
 
-	while (r == ret) {
-		if (!finishing) {
-			int written = write(conn->stream_pipes[1], data, len);
+	do {
+		if (ret == 2048) {
+			int written = write(conn->stream_pipes[1], data, len > ret ? ret : len);
 
 			/* When we're writing zero bytes already, we want to
 			 * zero ret properly for now, so that we'll go out for
@@ -700,37 +698,16 @@
 			 * zero bytes, but ret will be on its original value,
 			 * causing us to close the stream, and that's disaster
 			 * when more data are about to come yet. */
-			if (written > 0 || (!written && !len)) {
-				ret = written;
-				data += ret;
-				len -= ret;
+			if (written > 0) {
+				data += written;
+				len -= written;
 				if (*length_of_block > 0)
-					*length_of_block -= ret;
+					*length_of_block -= written;
 				if (!info->length)
-					finishing = 1;
+					ret = 65536;
+				else if (!len)
+					return output;
 			}
-
-			if (len) {
-				/* We assume that this is because full pipe. */
-				/* FIXME: We should probably handle errors as
-				 * well. --pasky */
-				ret = 4096; /* pipe capacity */
-			}
-		}
-		/* finishing could be changed above ;) */
-		if (finishing) {
-			/* Granularity of the final decompression. When we were
-			 * taking the decompressed content, we only took the
-			 * amount which we inserted there. When finishing, we
-			 * have to drain the rest from the beast. */
-			/* TODO: We should probably double the ret before trying
-			 * to read as well..? Would maybe make the progressive
-			 * displaying feeling better? --pasky */
-			ret = 65536;
-		}
-		if (ret < 4096) {
-			/* Not enough data, try in next round. */
-			return output;
 		}
 
 		if (!conn->stream) {
@@ -743,8 +720,9 @@
 		if (!output) break;
 
 		r = read_encoded(conn->stream, output + *new_len, ret);
+		
 		if (r > 0) *new_len += r;
-	}
+	} while (len || r == ret);
 
 	if (r < 0 && output) {
 		mem_free(output);
